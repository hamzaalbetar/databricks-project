ğŸš€ Databricks Data Lakehouse Project
ğŸ‘¨â€ğŸ’» Designed & Implemented by Hamza Albetar
ğŸ¯ Project Purpose

This project implements an end-to-end Data Engineering pipeline on Databricks using PySpark and Delta Lake.
It integrates data from two enterprise systems:

ğŸ“ CRM System

ğŸ“ ERP System

The goal is to transform raw CSV files into structured, analytics-ready tables following the Medallion Architecture (Bronze â†’ Silver â†’ Gold).

ğŸ§± Architecture Overview
ğŸ¥‰ Bronze Layer â€“ Raw Ingestion

Read original CSV files

Store as Delta tables

No transformations

Preserve source data as single source of truth

ğŸ¥ˆ Silver Layer â€“ Cleansed & Integrated

Data cleaning & normalization

Schema standardization

Integration between CRM and ERP domains

ğŸ“Œ Silver Tables

Silver.crm_cust_info

Silver.crm_prd_info

Silver.crm_sales_details

Silver.erp_cust_az12

Silver.erp_loc_z101

Silver.erp_az_prd_a1v2

ğŸ¥‡ Gold Layer â€“ Dimensional Model

Gold.dim_cust

Gold.dim_prd

Gold.fact_sales

Built using Star Schema to support analytics and BI workloads.

ğŸ›  Technologies Used

Databricks

Apache Spark â€“ PySpark

Delta Lake

GitHub

SQL Analytics

âš™ Pipeline Execution

Execution order:

Bronze notebooks

Silver notebooks

Gold notebooks

Validation stage

ğŸŸ¡ Current mode: Manual Databricks Job

âœ… Data Quality Rules

No null values in business keys

Unique customer & product IDs

Positive sales amounts

Referential integrity between fact and dimensions

ğŸ“¦ Project Outcome

Unified enterprise Data Lakehouse

Clean Delta architecture

Analytics-ready model

Ready foundation for BI & ML

ğŸ”® Future Improvements

Automated data quality framework

Scheduled workflow orchestration

CI/CD integration

Power BI dashboard on Gold layer
