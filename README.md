ğŸš€ Databricks Data Lakehouse Project
 Designed & Implemented by Hamza Albetar
ğŸ¯ Project Purpose

This project implements an end-to-end Data Engineering pipeline on Databricks using PySpark and Delta Lake.
It integrates data from two enterprise systems:

ğŸ“ CRM System

ğŸ“ ERP System

The goal is to transform raw CSV files into structured, analytics-ready tables following the Medallion Architecture (Bronze â†’ Silver â†’ Gold).

ğŸ§± Architecture Overview

ğŸ¥‰ Bronze Layer â€“ Raw Ingestion

Read original CSV files

Store as Delta tables

No transformations

Preserve source data as single source of truth

ğŸ¥ˆ Silver Layer â€“ Cleansed & Integrated

Data cleaning & normalization

Schema standardization

Integration between CRM and ERP domains

ğŸ“Œ Silver Tables

Silver_crm_cust_info

Silver_crm_prd_info

Silver_crm_sales_details

Silver_erp_cust_az12

Silver_erp_loc_a101

Silver_erp_px_cat_g1v2

ğŸ¥‡ Gold Layer â€“ Dimensional Model

Gold.dim_customers

Gold.dim_products

Gold.fact_sales

Built using Star Schema to support analytics and BI workloads.

ğŸ›  Technologies Used

Databricks

Apache Spark â€“ PySpark

Delta Lake

GitHub

SQL Analytics

âš™ Pipeline Execution

Execution order:

Bronze notebooks

Silver notebooks

Gold notebooks

Validation stage

ğŸŸ¡ Current mode: Manual Databricks Job

âœ… Data Quality Rules

No null values in business keys

Unique customer & product IDs

Positive sales amounts

Referential integrity between fact and dimensions

ğŸ“¦ Project Outcome

Unified enterprise Data Lakehouse

Clean Delta architecture

Analytics-ready model

Ready foundation for BI & ML

