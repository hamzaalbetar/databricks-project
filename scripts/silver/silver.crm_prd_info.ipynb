{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "556fa0b4-b392-4065-884a-0002f95b5796",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").table(\"workspace.bronze.crm_prd_info\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834e5e3c-8155-4c42-b229-8d7a2ea663a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca3f736-b339-4bd7-b852-447fee18281b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import substring\n",
    "\n",
    "df = df.withColumn(\"cut_id\", substring(\"prd_key\", 1, 5))\n",
    "cols = df.columns\n",
    "prd_key_idx = cols.index(\"prd_key\")\n",
    "# Remove 'cut_id' from the end if it exists\n",
    "if cols[-1] == \"cut_id\":\n",
    "    cols = cols[:-1]\n",
    "new_cols = cols[:prd_key_idx] + [\"cut_id\"] + cols[prd_key_idx:]\n",
    "df = df.select(*new_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8514ff74-ed2d-460e-80f4-3f249242df29",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length\n",
    "\n",
    "df = df.withColumn(\"prd_key\", substring(\"prd_key\", 7, length(\"prd_key\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4eb2ba41-eec9-4946-aa74-7eb0da3d9665",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "df = df.withColumn(\"prd_cost\", when(df[\"prd_cost\"].isNull(), 0).otherwise(df[\"prd_cost\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9c5e86b-fa54-4f8c-8f9d-20bd27cae65a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import upper, trim\n",
    "\n",
    "df = df.withColumn(\n",
    "    \"prd_line\",\n",
    "    when(upper(trim(df[\"prd_line\"])) == \"R\", \"road\")\n",
    "    .when(upper(trim(df[\"prd_line\"])) == \"M\", \"mountain\")\n",
    "    .when(upper(trim(df[\"prd_line\"])) == \"S\", \"touring\")\n",
    "    .when(upper(trim(df[\"prd_line\"])) == \"O\", \"other sales\")\n",
    "    .otherwise(\"n/a\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2f1b22b-df87-4f09-b0f4-622e310da24f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import lead\n",
    "\n",
    "w = Window.partitionBy(\"prd_key\").orderBy(\"prd_start_dt\")\n",
    "df = df.withColumn(\"prd_end_dt\", lead(\"prd_end_dt\").over(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0d0685d-874d-4d17-8b45-78280b687856",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.mode(\"overwrite\").saveAsTable(\"silver.crm_prd_info\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "silver.crm_prd_info",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
